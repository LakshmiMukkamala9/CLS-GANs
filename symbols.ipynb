{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fb5edf3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pickles'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickles\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Variable\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pickles'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickles\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c68bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text2ImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])])\n",
    "        \n",
    "        self.load_flower_dataset()\n",
    "\n",
    "    def load_flower_dataset(self):\n",
    "        # It will return two things : a list of image file names, a dictionary of 5 captions per image\n",
    "        # with image file name as the key of the dictionary and 5 values(captions) for each key.\n",
    "\n",
    "        print (\"------------------  Loading images  ------------------\")\n",
    "        filepath = os.path.join('C:\\Base\\\\', 'file_caption_map.pickle')\n",
    "        fileObject = open(filepath,'rb')  \n",
    "        filenames = pd.read_pickle(fileObject)\n",
    "        global img_files\n",
    "        img_files = np.array(list(filenames.keys()))\n",
    "        self.img_files = img_files\n",
    "\n",
    "        print('Load filenames from: %s (%d)' % (filepath, img_files.size))\n",
    "\n",
    "        print (\"------------------  Loading captions  ----------------\")\n",
    "        \n",
    "        self.img_captions = filenames                 \n",
    "        \n",
    "        print (\"---------------  Loading Skip-thought Model  ---------------\")\n",
    "        embedding_filename = '/file_caption_embedding.pickle'\n",
    "\n",
    "        with open('C:\\Base' + embedding_filename, 'rb') as f:\n",
    "            embeddings = pd.read_pickle(f)\n",
    "            self.encoded_captions =  embeddings   \n",
    "        \n",
    "        print (\"-------------  Encoding of image captions DONE  -------------\")\n",
    "\n",
    "    def read_image(self, image_file_name):\n",
    "#         print(image_file_name)\n",
    "        image = Image.open(os.path.join('C:\\Base\\images\\\\' + image_file_name))\n",
    "        # check its shape and reshape it to (64, 64, 3)\n",
    "        image = image.resize((64, 64))\n",
    "        return image\n",
    "\n",
    "    def get_false_img(self, index):\n",
    "        false_img_id = np.random.randint(len(self.img_files))\n",
    "        if false_img_id != index:\n",
    "            return self.img_files[false_img_id]\n",
    "\n",
    "        return self.get_false_img(index)\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return (img_files.size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        sample = {}\n",
    "#         print(self.image_transform(self.read_image(self.img_files)))\n",
    "#         print(self.image_transform(self.read_image(self.img_files[index])))\n",
    "#         print(self.image_transform(self.read_image(self.get_false_img(index))))\n",
    "        sample['true_imgs'] = self.image_transform(self.read_image(self.img_files[index]))\n",
    "        sample['false_imgs'] = self.image_transform(self.read_image(self.get_false_img(index)))\n",
    "        embeddings = self.encoded_captions[self.img_files[index]]\n",
    "        embedding_ix = random.randint(0, embeddings.shape[0]-1)\n",
    "        embedding = embeddings[embedding_ix, :]\n",
    "        sample['true_embed'] = torch.FloatTensor(embedding)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4730d035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1db5a33c",
   "metadata": {},
   "source": [
    "Cleaning data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f6a2228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "C:\\Users\\praja\\anaconda3\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import theano\n",
    "import theano.tensor as tensor\n",
    "\n",
    "import pickle as pkl\n",
    "import numpy\n",
    "import copy\n",
    "import nltk\n",
    "\n",
    "from collections import OrderedDict, defaultdict\n",
    "from scipy.linalg import norm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from numba import jit\n",
    "\n",
    "profile = False\n",
    "\n",
    "#-----------------------------------------------------------------------------#\n",
    "# Specify model and table locations here\n",
    "#-----------------------------------------------------------------------------#\n",
    "path_to_models = 'C:\\Base\\\\'\n",
    "path_to_tables = 'C:\\Base\\\\'\n",
    "#-----------------------------------------------------------------------------#\n",
    "\n",
    "path_to_umodel = path_to_models + 'uni_skip.npz'\n",
    "path_to_bmodel = path_to_models + 'bi_skip.npz'\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    Load the model with saved tables\n",
    "    \"\"\"\n",
    "    # Load model options\n",
    "    print('Loading model parameters...')\n",
    "    with open('%s.pkl'%path_to_umodel, 'rb') as f:\n",
    "        uoptions = pkl.load(f)\n",
    "    with open('%s.pkl'%path_to_bmodel, 'rb') as f:\n",
    "        boptions = pkl.load(f)\n",
    "\n",
    "    # Load parameters\n",
    "    uparams = init_params(uoptions)\n",
    "    uparams = load_params(path_to_umodel, uparams)\n",
    "    utparams = init_tparams(uparams)\n",
    "    bparams = init_params_bi(boptions)\n",
    "    bparams = load_params(path_to_bmodel, bparams)\n",
    "    btparams = init_tparams(bparams)\n",
    "\n",
    "    # Extractor functions\n",
    "    print('Compiling encoders...')\n",
    "    embedding, x_mask, ctxw2v = build_encoder(utparams, uoptions)\n",
    "    f_w2v = theano.function([embedding, x_mask], ctxw2v, name='f_w2v')\n",
    "    embedding, x_mask, ctxw2v = build_encoder_bi(btparams, boptions)\n",
    "    f_w2v2 = theano.function([embedding, x_mask], ctxw2v, name='f_w2v2')\n",
    "\n",
    "    # Tables\n",
    "    print('Loading tables...')\n",
    "    utable, btable = load_tables()\n",
    "\n",
    "    # Store everything we need in a dictionary\n",
    "    print('Packing up...')\n",
    "    model = {}\n",
    "    model['uoptions'] = uoptions\n",
    "    print('uoptions')\n",
    "    model['boptions'] = boptions\n",
    "    print('boptions')\n",
    "    model['utable'] = utable\n",
    "    print('utable')\n",
    "    model['btable'] = btable\n",
    "    print('btable')\n",
    "    model['f_w2v'] = f_w2v\n",
    "    print('f_w2v')\n",
    "    model['f_w2v2'] = f_w2v2\n",
    "    print('f_w2v2')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_tables():\n",
    "    \"\"\"\n",
    "    Load the tables\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    utable = numpy.load(path_to_tables + 'utable.npy', allow_pickle = True, encoding='bytes')\n",
    "    btable = numpy.load(path_to_tables + 'btable.npy', allow_pickle = True, encoding='bytes')\n",
    "    f = open(path_to_tables + 'dictionary.txt', 'rb')\n",
    "    for line in f:\n",
    "        words.append(line.decode('utf-8').strip())\n",
    "    f.close()\n",
    "    utable = OrderedDict(zip(words, utable))\n",
    "    btable = OrderedDict(zip(words, btable))\n",
    "    return utable, btable\n",
    "\n",
    "class Encoder(object):\n",
    "    \"\"\"\n",
    "    Sentence encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self._model = model\n",
    "\n",
    "    def encode(self, X, use_norm=True, verbose=True, batch_size=128, use_eos=False):\n",
    "        return encode(self._model, X, use_norm, verbose, batch_size, use_eos)\n",
    "    \n",
    "def encode(model, X, use_norm=True, verbose=True, batch_size=128, use_eos=False):\n",
    "    \n",
    "    X = preprocess(X)\n",
    "    # word dictionary and init\n",
    "    d = defaultdict(lambda : 0)\n",
    "    for w in model['utable'].keys():\n",
    "        d[w] = 1\n",
    "    ufeatures = numpy.zeros((len(X), model['uoptions']['dim']), dtype='float32')\n",
    "    bfeatures = numpy.zeros((len(X), 2 * model['boptions']['dim']), dtype='float32')\n",
    "\n",
    "    # length dictionary\n",
    "    ds = defaultdict(list)\n",
    "    captions = [s.split() for s in X]\n",
    "    for i,s in enumerate(captions):\n",
    "        ds[len(s)].append(i)\n",
    "#     print(ds)\n",
    "\n",
    "    # Get features. This encodes by length, in order to avoid wasting computation\n",
    "    for k in ds.keys():\n",
    "        if verbose:\n",
    "            print(k)\n",
    "#         print(k, ds.keys())\n",
    "        numbatches = len(ds[k])\n",
    "#         print(numbatches)\n",
    "        for minibatch in range(numbatches):\n",
    "            caps = ds[k][minibatch::numbatches]\n",
    "\n",
    "            if use_eos:\n",
    "                uembedding = numpy.zeros((k+1, len(caps), model['uoptions']['dim_word']), dtype='float32')\n",
    "                bembedding = numpy.zeros((k+1, len(caps), model['boptions']['dim_word']), dtype='float32')\n",
    "            else:\n",
    "                uembedding = numpy.zeros((k, len(caps), model['uoptions']['dim_word']), dtype='float32')\n",
    "                bembedding = numpy.zeros((k, len(caps), model['boptions']['dim_word']), dtype='float32')\n",
    "            for ind, c in enumerate(caps):\n",
    "                caption = captions[c]\n",
    "                for j in range(len(caption)):\n",
    "                    if d[caption[j]] > 0:\n",
    "                        uembedding[j,ind] = model['utable'][caption[j]]\n",
    "                        bembedding[j,ind] = model['btable'][caption[j]]\n",
    "                    else:\n",
    "                        uembedding[j,ind] = model['utable']['UNK']\n",
    "                        bembedding[j,ind] = model['btable']['UNK']\n",
    "                if use_eos:\n",
    "                    uembedding[-1,ind] = model['utable']['<eos>']\n",
    "                    bembedding[-1,ind] = model['btable']['<eos>']\n",
    "            if use_eos:\n",
    "                uff = model['f_w2v'](uembedding, numpy.ones((len(caption)+1,len(caps)), dtype='float32'))\n",
    "                bff = model['f_w2v2'](bembedding, numpy.ones((len(caption)+1,len(caps)), dtype='float32'))\n",
    "            else:\n",
    "                uff = model['f_w2v'](uembedding, numpy.ones((len(caption),len(caps)), dtype='float32'))\n",
    "                bff = model['f_w2v2'](bembedding, numpy.ones((len(caption),len(caps)), dtype='float32'))\n",
    "            if use_norm:\n",
    "                for j in range(len(uff)):\n",
    "                    uff[j] /= norm(uff[j])\n",
    "                    bff[j] /= norm(bff[j])\n",
    "            for ind, c in enumerate(caps):\n",
    "                ufeatures[c] = uff[ind]\n",
    "                bfeatures[c] = bff[ind]\n",
    "    features = numpy.c_[ufeatures, bfeatures]\n",
    "    return features\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Preprocess text for encoder\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    for t in text:\n",
    "        sents = sent_detector.tokenize(t)\n",
    "        result = ''\n",
    "        for s in sents:\n",
    "            tokens = word_tokenize(s)\n",
    "            result += ' ' + ' '.join(tokens)\n",
    "        X.append(result)\n",
    "    return X\n",
    "\n",
    "\n",
    "def nn(model, text, vectors, query, k=5):\n",
    "    \"\"\"\n",
    "    Return the nearest neighbour sentences to query\n",
    "    text: list of sentences\n",
    "    vectors: the corresponding representations for text\n",
    "    query: a string to search\n",
    "    \"\"\"\n",
    "    qf = encode(model, [query])\n",
    "    qf /= norm(qf)\n",
    "    scores = numpy.dot(qf, vectors.T).flatten()\n",
    "    sorted_args = numpy.argsort(scores)[::-1]\n",
    "    sentences = [text[a] for a in sorted_args[:k]]\n",
    "    print('QUERY: ' + query)\n",
    "    print('NEAREST: ')\n",
    "    for i, s in enumerate(sentences):\n",
    "        print(s, sorted_args[i])\n",
    "\n",
    "\n",
    "def word_features(table):\n",
    "    \"\"\"\n",
    "    Extract word features into a normalized matrix\n",
    "    \"\"\"\n",
    "    features = numpy.zeros((len(table), 620), dtype='float32')\n",
    "    keys = table.keys()\n",
    "    for i in range(len(table)):\n",
    "        f = table[keys[i]]\n",
    "        features[i] = f / norm(f)\n",
    "    return features\n",
    "\n",
    "\n",
    "def nn_words(table, wordvecs, query, k=10):\n",
    "    \"\"\"\n",
    "    Get the nearest neighbour words\n",
    "    \"\"\"\n",
    "    keys = table.keys()\n",
    "    qf = table[query]\n",
    "    scores = numpy.dot(qf, wordvecs.T).flatten()\n",
    "    sorted_args = numpy.argsort(scores)[::-1]\n",
    "    words = [keys[a] for a in sorted_args[:k]]\n",
    "    print('QUERY: ' + query)\n",
    "    print('NEAREST: ')\n",
    "    for i, w in enumerate(words):\n",
    "        print(w)\n",
    "\n",
    "\n",
    "def _p(pp, name):\n",
    "    \"\"\"\n",
    "    make prefix-appended name\n",
    "    \"\"\"\n",
    "    return '%s_%s'%(pp, name)\n",
    "\n",
    "\n",
    "def init_tparams(params):\n",
    "    \"\"\"\n",
    "    initialize Theano shared variables according to the initial parameters\n",
    "    \"\"\"\n",
    "    tparams = OrderedDict()\n",
    "    for kk, pp in params.items():\n",
    "        tparams[kk] = theano.shared(params[kk], name=kk)\n",
    "    return tparams\n",
    "\n",
    "\n",
    "def load_params(path, params):\n",
    "    \"\"\"\n",
    "    load parameters\n",
    "    \"\"\"\n",
    "    pp = numpy.load(path)\n",
    "    for kk, vv in params.items():\n",
    "        if kk not in pp:\n",
    "            warnings.warn('%s is not in the archive'%kk)\n",
    "            continue\n",
    "        params[kk] = pp[kk]\n",
    "    return params\n",
    "\n",
    "\n",
    "# layers: 'name': ('parameter initializer', 'feedforward')\n",
    "layers = {'gru': ('param_init_gru', 'gru_layer')}\n",
    "\n",
    "def get_layer(name):\n",
    "    fns = layers[name]\n",
    "    return (eval(fns[0]), eval(fns[1]))\n",
    "\n",
    "\n",
    "def init_params(options):\n",
    "    \"\"\"\n",
    "    initialize all parameters needed for the encoder\n",
    "    \"\"\"\n",
    "    params = OrderedDict()\n",
    "\n",
    "    # embedding\n",
    "    params['Wemb'] = norm_weight(options['n_words_src'], options['dim_word'])\n",
    "\n",
    "    # encoder: GRU\n",
    "    params = get_layer(options['encoder'])[0](options, params, prefix='encoder',\n",
    "                                              nin=options['dim_word'], dim=options['dim'])\n",
    "    return params\n",
    "\n",
    "\n",
    "def init_params_bi(options):\n",
    "    \"\"\"\n",
    "    initialize all paramters needed for bidirectional encoder\n",
    "    \"\"\"\n",
    "    params = OrderedDict()\n",
    "\n",
    "    # embedding\n",
    "    params['Wemb'] = norm_weight(options['n_words_src'], options['dim_word'])\n",
    "\n",
    "    # encoder: GRU\n",
    "    params = get_layer(options['encoder'])[0](options, params, prefix='encoder',\n",
    "                                              nin=options['dim_word'], dim=options['dim'])\n",
    "    params = get_layer(options['encoder'])[0](options, params, prefix='encoder_r',\n",
    "                                              nin=options['dim_word'], dim=options['dim'])\n",
    "    return params\n",
    "\n",
    "\n",
    "def build_encoder(tparams, options):\n",
    "    \"\"\"\n",
    "    build an encoder, given pre-computed word embeddings\n",
    "    \"\"\"\n",
    "    # word embedding (source)\n",
    "    embedding = tensor.tensor3('embedding', dtype='float32')\n",
    "    x_mask = tensor.matrix('x_mask', dtype='float32')\n",
    "\n",
    "    # encoder\n",
    "    proj = get_layer(options['encoder'])[1](tparams, embedding, options,\n",
    "                                            prefix='encoder',\n",
    "                                            mask=x_mask)\n",
    "    ctx = proj[0][-1]\n",
    "\n",
    "    return embedding, x_mask, ctx\n",
    "\n",
    "\n",
    "def build_encoder_bi(tparams, options):\n",
    "    \"\"\"\n",
    "    build bidirectional encoder, given pre-computed word embeddings\n",
    "    \"\"\"\n",
    "    # word embedding (source)\n",
    "    embedding = tensor.tensor3('embedding', dtype='float32')\n",
    "    embeddingr = embedding[::-1]\n",
    "    x_mask = tensor.matrix('x_mask', dtype='float32')\n",
    "    xr_mask = x_mask[::-1]\n",
    "\n",
    "    # encoder\n",
    "    proj = get_layer(options['encoder'])[1](tparams, embedding, options,\n",
    "                                            prefix='encoder',\n",
    "                                            mask=x_mask)\n",
    "    projr = get_layer(options['encoder'])[1](tparams, embeddingr, options,\n",
    "                                             prefix='encoder_r',\n",
    "                                             mask=xr_mask)\n",
    "\n",
    "    ctx = tensor.concatenate([proj[0][-1], projr[0][-1]], axis=1)\n",
    "\n",
    "    return embedding, x_mask, ctx\n",
    "\n",
    "\n",
    "# some utilities\n",
    "def ortho_weight(ndim):\n",
    "    W = numpy.random.randn(ndim, ndim)\n",
    "    u, s, v = numpy.linalg.svd(W)\n",
    "    return u.astype('float32')\n",
    "\n",
    "\n",
    "def norm_weight(nin,nout=None, scale=0.1, ortho=True):\n",
    "    if nout == None:\n",
    "        nout = nin\n",
    "    if nout == nin and ortho:\n",
    "        W = ortho_weight(nin)\n",
    "    else:\n",
    "        W = numpy.random.uniform(low=-scale, high=scale, size=(nin, nout))\n",
    "    return W.astype('float32')\n",
    "\n",
    "\n",
    "def param_init_gru(options, params, prefix='gru', nin=None, dim=None):\n",
    "    \"\"\"\n",
    "    parameter init for GRU\n",
    "    \"\"\"\n",
    "    if nin == None:\n",
    "        nin = options['dim_proj']\n",
    "    if dim == None:\n",
    "        dim = options['dim_proj']\n",
    "    W = numpy.concatenate([norm_weight(nin,dim),\n",
    "                           norm_weight(nin,dim)], axis=1)\n",
    "    params[_p(prefix,'W')] = W\n",
    "    params[_p(prefix,'b')] = numpy.zeros((2 * dim,)).astype('float32')\n",
    "    U = numpy.concatenate([ortho_weight(dim),\n",
    "                           ortho_weight(dim)], axis=1)\n",
    "    params[_p(prefix,'U')] = U\n",
    "\n",
    "    Wx = norm_weight(nin, dim)\n",
    "    params[_p(prefix,'Wx')] = Wx\n",
    "    Ux = ortho_weight(dim)\n",
    "    params[_p(prefix,'Ux')] = Ux\n",
    "    params[_p(prefix,'bx')] = numpy.zeros((dim,)).astype('float32')\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def gru_layer(tparams, state_below, options, prefix='gru', mask=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Forward pass through GRU layer\n",
    "    \"\"\"\n",
    "    nsteps = state_below.shape[0]\n",
    "    if state_below.ndim == 3:\n",
    "        n_samples = state_below.shape[1]\n",
    "    else:\n",
    "        n_samples = 1\n",
    "\n",
    "    dim = tparams[_p(prefix,'Ux')].shape[1]\n",
    "\n",
    "    if mask == None:\n",
    "        mask = tensor.alloc(1., state_below.shape[0], 1)\n",
    "\n",
    "    def _slice(_x, n, dim):\n",
    "        if _x.ndim == 3:\n",
    "            return _x[:, :, n*dim:(n+1)*dim]\n",
    "        return _x[:, n*dim:(n+1)*dim]\n",
    "\n",
    "    state_below_ = tensor.dot(state_below, tparams[_p(prefix, 'W')]) + tparams[_p(prefix, 'b')]\n",
    "    state_belowx = tensor.dot(state_below, tparams[_p(prefix, 'Wx')]) + tparams[_p(prefix, 'bx')]\n",
    "    U = tparams[_p(prefix, 'U')]\n",
    "    Ux = tparams[_p(prefix, 'Ux')]\n",
    "\n",
    "    def _step_slice(m_, x_, xx_, h_, U, Ux):\n",
    "        preact = tensor.dot(h_, U)\n",
    "        preact += x_\n",
    "\n",
    "        r = tensor.nnet.sigmoid(_slice(preact, 0, dim))\n",
    "        u = tensor.nnet.sigmoid(_slice(preact, 1, dim))\n",
    "\n",
    "        preactx = tensor.dot(h_, Ux)\n",
    "        preactx = preactx * r\n",
    "        preactx = preactx + xx_\n",
    "\n",
    "        h = tensor.tanh(preactx)\n",
    "\n",
    "        h = u * h_ + (1. - u) * h\n",
    "        h = m_[:,None] * h + (1. - m_)[:,None] * h_\n",
    "\n",
    "        return h\n",
    "\n",
    "    seqs = [mask, state_below_, state_belowx]\n",
    "    _step = _step_slice\n",
    "\n",
    "    rval, updates = theano.scan(_step,\n",
    "                                sequences=seqs,\n",
    "                                outputs_info = [tensor.alloc(0., n_samples, dim)],\n",
    "                                non_sequences = [tparams[_p(prefix, 'U')],\n",
    "                                                 tparams[_p(prefix, 'Ux')]],\n",
    "                                name=_p(prefix, '_layers'),\n",
    "                                n_steps=nsteps,\n",
    "                                profile=profile,\n",
    "                                strict=True)\n",
    "    rval = [rval]\n",
    "    return rval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db5cd912",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model = load_model()\n",
    "# encoder = Encoder(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "059c0b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model parameters...\n",
      "Compiling encoders...\n",
      "Loading tables...\n",
      "Packing up...\n",
      "uoptions\n",
      "boptions\n",
      "utable\n",
      "btable\n",
      "f_w2v\n",
      "f_w2v2\n",
      "load model\n",
      "model done\n"
     ]
    }
   ],
   "source": [
    "model = load_model()\n",
    "print('load model')\n",
    "encoder = Encoder(model)\n",
    "print('model done')\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "# file_caption = defaultdict(list)\n",
    "# with open('C:\\Base\\caption.txt') as fp:\n",
    "#     line = fp.readline()\n",
    "#     # you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "#     #line = [x.strip() for x in line]\n",
    "#     while(line):\n",
    "#         line = line.strip()\n",
    "#         print(line)\n",
    "#         file_name, caption =  line.split(',')\n",
    "#         file_caption[file_name].append(caption)\n",
    "#         line = fp.readline()\n",
    "\n",
    "# print(len(file_caption))\n",
    "# print(file_caption['1.png'])\n",
    "# print(file_caption['3.png'])\n",
    "# file_Name = \"file_caption_map.pickle\"\n",
    "# # open the file for writing\n",
    "# fileObject = open(file_Name,'wb') \n",
    "# pickle.dump(file_caption,fileObject)   \n",
    "# # here we close the fileObject\n",
    "# fileObject.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da788af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\praja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "from numba import jit, cuda\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "  \n",
    "\n",
    "file_Name = \"file_caption_map.pickle\"\n",
    "# we open the file for reading\n",
    "fileObject = open(file_Name,'rb')  \n",
    "# load the object from the file into var b\n",
    "file_caption = pickle.load(fileObject)  \n",
    "\n",
    "file_emedding = defaultdict(list)\n",
    "for key, value in file_caption.items():\n",
    "    file_emedding[key] = encoder.encode(value, verbose=False)\n",
    "#     print(key, value)\n",
    "#     print(len(file_emedding[key]))\n",
    "#     print(file_emedding[key])\n",
    "    print('-----------------------')\n",
    "    \n",
    "print(len(file_emedding))\n",
    "print(file_emedding['1.png'])\n",
    "print(file_emedding['3.png'])\n",
    "\n",
    "file_Name = \"file_caption_embedding.pickle\"\n",
    "# open the file for writing\n",
    "fileObject = open(file_Name,'wb') \n",
    "\n",
    "pickle.dump(file_emedding,fileObject)   \n",
    "\n",
    "# here we close the fileObject\n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d353199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, batch_size, img_size, z_dim, text_embed_dim, reduced_text_dim):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.z_dim = z_dim\n",
    "        self.text_embed_dim = text_embed_dim\n",
    "        self.reduced_text_dim = reduced_text_dim\n",
    "\n",
    "        self.reduced_text_dim = nn.Linear(text_embed_dim, reduced_text_dim)\n",
    "        self.concat = nn.Linear(z_dim + reduced_text_dim, 64 * 8 * 4 * 4)\n",
    "\n",
    "        # Defining the generator network architecture\n",
    "        self.d_net = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 1, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, text, z):\n",
    "        \"\"\" Given a caption embedding and latent variable z(noise), generate an image\n",
    "        Arguments\n",
    "        ---------\n",
    "        text : torch.FloatTensor\n",
    "            Output of the skipthought embedding model for the caption\n",
    "            text.size() = (batch_size, text_embed_dim)\n",
    "        z : torch.FloatTensor\n",
    "            Latent variable or noise\n",
    "            z.size() = (batch_size, z_dim)\n",
    "        --------\n",
    "        Returns\n",
    "        --------\n",
    "        output : An image of shape (64, 64, 3)\n",
    "        \"\"\"\n",
    "        reduced_text = self.reduced_text_dim(text)  # (batch_size, reduced_text_dim)\n",
    "        concat = torch.cat((reduced_text, z), 1)  # (batch_size, reduced_text_dim + z_dim)\n",
    "        concat = self.concat(concat)  # (batch_size, 64*8*4*4)\n",
    "        concat = concat.view(-1, 64 * 8, 4, 4)  # (batch_size, 4, 4, 64*8)\n",
    "        \n",
    "        d_net_out = self.d_net(concat)  # (batch_size, 64, 64, 3)\n",
    "        output = d_net_out / 2. + 0.5   # (batch_size, 64, 64, 3)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a45c9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, batch_size, img_size, text_embed_dim, text_reduced_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.in_channels = 1\n",
    "        self.text_embed_dim = text_embed_dim\n",
    "        self.text_reduced_dim_val = text_reduced_dim\n",
    "\n",
    "        # Defining the discriminator network architecture\n",
    "        self.d_net = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        # output_dim = (batch_size, 4, 4, 512)\n",
    "        # text.size() = (batch_size, text_embed_dim)\n",
    "\n",
    "        # Defining a linear layer to reduce the dimensionality of caption embedding\n",
    "        # from text_embed_dim to text_reduced_dim\n",
    "        self.text_reduced_dim = nn.Linear(self.text_embed_dim, self.text_reduced_dim_val)\n",
    "\n",
    "        self.cat_net = nn.Sequential(\n",
    "            nn.Conv2d(512 + self.text_reduced_dim_val, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        self.linear = nn.Linear(2 * 2 * 512, 1)\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        \"\"\" Given the image and its caption embedding, predict whether the image\n",
    "        is real or fake.\n",
    "        Arguments\n",
    "        ---------\n",
    "        image : torch.FloatTensor\n",
    "            image.size() = (batch_size, 64, 64, 3)\n",
    "        text : torch.FloatTensor\n",
    "            Output of the skipthought embedding model for the caption\n",
    "            text.size() = (batch_size, text_embed_dim)\n",
    "        --------\n",
    "        Returns\n",
    "        --------\n",
    "        output : Probability for the image being real/fake\n",
    "        logit : Final score of the discriminator\n",
    "        \"\"\"\n",
    "\n",
    "#         print('text', text.shape)\n",
    "#         print('image ', image.shape)\n",
    "        d_net_out = self.d_net(image)  # (batch_size, 4, 4, 512)\n",
    "#         print('d_net_out ', d_net_out.shape)\n",
    "        text_reduced = self.text_reduced_dim(text)  # (batch_size, text_reduced_dim)\n",
    "#         print('text_reduced original ', text_reduced.shape)\n",
    "        \n",
    "        text_reduced = text_reduced.unsqueeze(1)  # (batch_size, 1, text_reduced_dim)\n",
    "#         print('text_reduced1 ', text_reduced.shape)\n",
    "        text_reduced = text_reduced.unsqueeze(2)  # (batch_size, 1, 1, text_reduced_dim)\n",
    "#         print('text_reduced2 ', text_reduced.shape)\n",
    "        text_reduced = text_reduced.transpose(3, 1)\n",
    "#         print('text_reduced3 ', text_reduced.shape)\n",
    "        \n",
    "        text_reduced = text_reduced.expand(-1, -1,d_net_out.shape[2],d_net_out.shape[3])\n",
    "#         print('text_reduced4 ', text_reduced.shape)\n",
    "        \n",
    "        concat_out = torch.cat((d_net_out, text_reduced), 1)  # (1, 4, 4, 512+text_reduced_dim)\n",
    "        \n",
    "        logit = self.cat_net(concat_out)\n",
    "#         print('old logit is ', logit.shape)\n",
    "        logit = logit.view(-1, 512*2*2)\n",
    "#         print('new logit is ', logit.shape) \n",
    "        output = F.sigmoid(self.linear(logit))\n",
    "#         print('output shape', output.shape)\n",
    "        output = output.view(-1, 1).squeeze(1)\n",
    "        #print('output shape', output.shape)\n",
    "        return output, logit\n",
    "\n",
    "        d_net_out = self.d_net(image)  # (batch_size, 4, 4, 512)\n",
    "        text_reduced = self.text_reduced_dim(text)  # (batch_size, text_reduced_dim)\n",
    "        \n",
    "        text_reduced = text_reduced.unsqueeze(1)  # (batch_size, 1, text_reduced_dim)\n",
    "        text_reduced = text_reduced.unsqueeze(2)  # (batch_size, 1, 1, text_reduced_dim)\n",
    "        text_reduced = text_reduced.transpose(3, 1)\n",
    "        \n",
    "        text_reduced = text_reduced.expand(-1, -1,d_net_out.shape[2],d_net_out.shape[3])\n",
    "        \n",
    "        concat_out = torch.cat((d_net_out, text_reduced), 1)  # (1, 4, 4, 512+text_reduced_dim)\n",
    "        \n",
    "        logit = self.cat_net(concat_out)\n",
    "        logit = logit.view(-1, 512*2*2)\n",
    "        output = F.sigmoid(self.linear(logit))\n",
    "        output = output.view(-1, 1).squeeze(1)\n",
    "        #print('output shape', output.shape)\n",
    "        return output, logit\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52105193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import  autograd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "class Concat_embed(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, projected_embed_dim):\n",
    "        super(Concat_embed, self).__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(in_features=embed_dim, out_features=projected_embed_dim),\n",
    "            nn.BatchNorm1d(num_features=projected_embed_dim),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "            )\n",
    "\n",
    "    def forward(self, inp, embed):\n",
    "        projected_embed = self.projection(embed)\n",
    "        replicated_embed = projected_embed.repeat(4, 4, 1, 1).permute(2,  3, 0, 1)\n",
    "        hidden_concat = torch.cat([inp, replicated_embed], 1)\n",
    "\n",
    "        return hidden_concat\n",
    "\n",
    "\n",
    "class minibatch_discriminator(nn.Module):\n",
    "    def __init__(self, num_channels, B_dim, C_dim):\n",
    "        super(minibatch_discriminator, self).__init__()\n",
    "        self.B_dim = B_dim\n",
    "        self.C_dim =C_dim\n",
    "        self.num_channels = num_channels\n",
    "        T_init = torch.randn(num_channels * 4 * 4, B_dim * C_dim) * 0.1\n",
    "        self.T_tensor = nn.Parameter(T_init, requires_grad=True)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        inp = inp.view(-1, self.num_channels * 4 * 4)\n",
    "        M = inp.mm(self.T_tensor)\n",
    "        M = M.view(-1, self.B_dim, self.C_dim)\n",
    "\n",
    "        op1 = M.unsqueeze(3)\n",
    "        op2 = M.permute(1, 2, 0).unsqueeze(0)\n",
    "\n",
    "        output = torch.sum(torch.abs(op1 - op2), 2)\n",
    "        output = torch.sum(torch.exp(-output), 2)\n",
    "        output = output.view(M.size(0), -1)\n",
    "\n",
    "        output = torch.cat((inp, output), 1)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class Utils(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def smooth_label(tensor, offset):\n",
    "        return tensor + offset\n",
    "\n",
    "    @staticmethod\n",
    "\n",
    "    # based on:  https://github.com/caogang/wgan-gp/blob/master/gan_cifar10.py\n",
    "    def compute_GP(netD, real_data, real_embed, fake_data, LAMBDA):\n",
    "        BATCH_SIZE = real_data.size(0)\n",
    "        alpha = torch.rand(BATCH_SIZE, 1)\n",
    "        alpha = alpha.expand(BATCH_SIZE, int(real_data.nelement() / BATCH_SIZE)).contiguous().view(BATCH_SIZE, 3, 64, 64)\n",
    "        alpha = alpha.cuda()\n",
    "\n",
    "        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "\n",
    "        interpolates = interpolates.cuda()\n",
    "\n",
    "        interpolates = autograd.Variable(interpolates, requires_grad=True)\n",
    "\n",
    "        disc_interpolates, _ = netD(interpolates, real_embed)\n",
    "\n",
    "        gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                                  grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n",
    "                                  create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n",
    "\n",
    "        return gradient_penalty\n",
    "\n",
    "    @staticmethod\n",
    "    def save_checkpoint(netD, netG, dir_path, subdir_path, epoch):\n",
    "        path =  os.path.join(dir_path, subdir_path)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        torch.save(netD.state_dict(), '{0}/disc_{1}.pth'.format(path, epoch))\n",
    "        torch.save(netG.state_dict(), '{0}/gen_{1}.pth'.format(path, epoch))\n",
    "\n",
    "    @staticmethod\n",
    "    def weights_init(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Conv') != -1:\n",
    "            m.weight.data.normal_(0.0, 0.02)\n",
    "        elif classname.find('BatchNorm') != -1:\n",
    "            m.weight.data.normal_(1.0, 0.02)\n",
    "            m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05c46591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "class GAN_CLS(object):\n",
    "    def __init__(self, args, data_loader, SUPERVISED=True):\n",
    "        \"\"\"\n",
    "        Arguments :\n",
    "        ----------\n",
    "        args : Arguments defined in Argument Parser\n",
    "        data_loader = An instance of class DataLoader for loading our dataset in batches\n",
    "        SUPERVISED :\n",
    "        \"\"\"\n",
    "        config = args\n",
    "        self.data_loader = data_loader\n",
    "        self.num_epochs = args.num_epochs\n",
    "        self.batch_size = args.batch_size\n",
    "\n",
    "        self.log_step = config.log_step\n",
    "        self.sample_step = config.sample_step\n",
    "\n",
    "        self.log_dir = args.log_dir\n",
    "        self.checkpoint_dir = args.checkpoint_dir\n",
    "        self.sample_dir = config.sample_dir\n",
    "        self.final_model = args.final_model\n",
    "\n",
    "        self.dataset = args.dataset\n",
    "        #self.model_name = args.model_name\n",
    "\n",
    "        self.img_size = args.img_size\n",
    "        self.z_dim = args.z_dim\n",
    "        self.text_embed_dim = args.text_embed_dim\n",
    "        self.text_reduced_dim = args.text_reduced_dim\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.beta1 = args.beta1\n",
    "        self.beta2 = args.beta2\n",
    "        self.l1_coeff = args.l1_coeff\n",
    "        self.resume_epoch = args.resume_epoch\n",
    "        self.SUPERVISED = SUPERVISED\n",
    "\n",
    "        # Logger setting\n",
    "        self.logger = logging.getLogger('__name__')\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        self.formatter = logging.Formatter('%(asctime)s:%(levelname)s:%(message)s')\n",
    "        self.file_handler = logging.FileHandler(self.log_dir+'/file.log')\n",
    "        self.file_handler.setFormatter(self.formatter)\n",
    "        self.logger.addHandler(self.file_handler)\n",
    "\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\" A function of defining following instances :\n",
    "        -----  Generator\n",
    "        -----  Discriminator\n",
    "        -----  Optimizer for Generator\n",
    "        -----  Optimizer for Discriminator\n",
    "        -----  Defining Loss functions\n",
    "        \"\"\"\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        #\t\t\t\t\t\t1. Network Initialization\n",
    "        # ---------------------------------------------------------------------\n",
    "        self.gen = Generator(batch_size=self.batch_size,\n",
    "            img_size=self.img_size,\n",
    "            z_dim=self.z_dim,\n",
    "            text_embed_dim=self.text_embed_dim,\n",
    "            reduced_text_dim=self.text_reduced_dim)\n",
    "\n",
    "        self.disc = Discriminator(batch_size=self.batch_size,\n",
    "                                  img_size=self.img_size,\n",
    "                                  text_embed_dim=self.text_embed_dim,\n",
    "                                  text_reduced_dim=self.text_reduced_dim)\n",
    "\n",
    "        self.gen_optim = optim.Adam(self.gen.parameters(),\n",
    "                                    lr=self.learning_rate,\n",
    "                                    betas=(self.beta1, self.beta2))\n",
    "\n",
    "        self.disc_optim = optim.Adam(self.disc.parameters(),\n",
    "                                     lr=self.learning_rate,\n",
    "                                     betas=(self.beta1, self.beta2))\n",
    "\n",
    "        self.cls_gan_optim = optim.Adam(itertools.chain(self.gen.parameters(),\n",
    "                                                        self.disc.parameters()),\n",
    "                                        lr=self.learning_rate,\n",
    "                                        betas=(self.beta1, self.beta2))\n",
    "\n",
    "        print ('-------------  Generator Model Info  ---------------')\n",
    "        self.print_network(self.gen, 'G')\n",
    "        print ('------------------------------------------------')\n",
    "\n",
    "        print ('-------------  Discriminator Model Info  ---------------')\n",
    "        self.print_network(self.disc, 'D')\n",
    "        print ('------------------------------------------------')\n",
    "\n",
    "        self.gen.cuda()\n",
    "        self.disc.cuda()\n",
    "        self.criterion = nn.BCELoss().cuda()\n",
    "        self.l1loss = nn.L1Loss().cuda()\n",
    "        self.l2loss = nn.MSELoss().cuda()\n",
    "        # self.CE_loss = nn.CrossEntropyLoss().cuda()\n",
    "        # self.MSE_loss = nn.MSELoss().cuda()\n",
    "        self.gen.train()\n",
    "        self.disc.train()\n",
    "\n",
    "    def print_network(self, model, name):\n",
    "        \"\"\" A function for printing total number of model parameters \"\"\"\n",
    "        num_params = 0\n",
    "        for p in model.parameters():\n",
    "            num_params += p.numel()\n",
    "\n",
    "        print(model)\n",
    "        print(name)\n",
    "        print(\"Total number of parameters: {}\".format(num_params))\n",
    "\n",
    "    def load_checkpoints(self, resume_epoch):\n",
    "        \"\"\"Restore the trained generator and discriminator.\"\"\"\n",
    "        print('Loading the trained models from step {}...'.format(resume_epoch))\n",
    "        G_path = os.path.join(self.checkpoint_dir, '{}-G.ckpt'.format(resume_epoch))\n",
    "        D_path = os.path.join(self.checkpoint_dir, '{}-D.ckpt'.format(resume_epoch))\n",
    "        self.gen.load_state_dict(torch.load(G_path, map_location=lambda storage, loc: storage))\n",
    "        self.disc.load_state_dict(torch.load(D_path, map_location=lambda storage, loc: storage))\n",
    "\n",
    "    def save_img_results(self, data_img, fake, epoch, image_dir):\n",
    "        num = 64\n",
    "        fake = fake[0:num]\n",
    "        # data_img is changed to [0,1]\n",
    "        if data_img is not None:\n",
    "            data_img = data_img[0:num]\n",
    "            vutils.save_image(data_img, '%s/real_samples_epoch_%03d.png' % (image_dir, epoch), normalize=True)\n",
    "            # fake.data is still [-1, 1]\n",
    "            vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' %(image_dir, epoch), normalize=True)\n",
    "        else:\n",
    "            vutils.save_image(\n",
    "                fake.data, '%s/lr_fake_samples_epoch_%03d.png' %\n",
    "                (image_dir, epoch), normalize=True)\n",
    "\n",
    "    def train_model(self):\n",
    "        fixed_noise = Variable(torch.randn(64, self.z_dim)).cuda()\n",
    "        data_loader = self.data_loader\n",
    "\n",
    "        start_epoch = 0\n",
    "        if self.resume_epoch:\n",
    "            start_epoch = self.resume_epoch\n",
    "            self.load_checkpoints(self.resume_epoch)\n",
    "\n",
    "        print ('---------------  Model Training Started  ---------------')\n",
    "        start_time = time.time()\n",
    "        log = \"\"\n",
    "        for epoch in range(start_epoch, self.num_epochs):\n",
    "            start_t = time.time()\n",
    "            for idx, batch in enumerate(data_loader):\n",
    "                true_imgs = batch['true_imgs']\n",
    "                true_embed = batch['true_embed']\n",
    "                false_imgs = batch['false_imgs']\n",
    "\n",
    "                real_labels = torch.ones(true_imgs.size(0))\n",
    "                fake_labels = torch.zeros(true_imgs.size(0))\n",
    "                \n",
    "                smooth_real_labels = torch.FloatTensor(Utils.smooth_label(real_labels.numpy(), -0.1))\n",
    "\n",
    "                true_imgs = Variable(true_imgs.float()).cuda()\n",
    "                true_embed = Variable(true_embed.float()).cuda()\n",
    "                false_imgs = Variable(false_imgs.float()).cuda()\n",
    "\n",
    "                real_labels = Variable(real_labels).cuda()\n",
    "                smooth_real_labels = Variable(smooth_real_labels).cuda()\n",
    "                fake_labels = Variable(fake_labels).cuda()\n",
    "\n",
    "                # ---------------------------------------------------------------\n",
    "                #                   2. Training the discriminator\n",
    "                # ---------------------------------------------------------------\n",
    "                self.disc.zero_grad()\n",
    "                true_out, true_logit = self.disc(true_imgs, true_embed)\n",
    "                false_out, false_logit = self.disc(false_imgs, true_embed)\n",
    "                disc_loss = self.criterion(true_out, smooth_real_labels) + self.criterion(false_out, fake_labels)\n",
    "\n",
    "                noise = Variable(torch.randn(true_imgs.size(0), self.z_dim)).cuda()\n",
    "                fake_imgs = self.gen(true_embed, noise)\n",
    "                false_out, _ = self.disc(fake_imgs, true_embed)\n",
    "                disc_loss = disc_loss + self.criterion(false_out, fake_labels)\n",
    "\n",
    "                disc_loss.backward()\n",
    "                self.disc_optim.step()\n",
    "\n",
    "\n",
    "                # ---------------------------------------------------------------\n",
    "                # \t\t\t\t\t  3. Training the generator\n",
    "                # ---------------------------------------------------------------\n",
    "                self.gen.zero_grad()\n",
    "                \n",
    "                z = Variable(torch.randn(true_imgs.size(0), self.z_dim)).cuda()\n",
    "                fake_imgs = self.gen(true_embed, z)\n",
    "                fake_out, fake_logit = self.disc(fake_imgs, true_embed)\n",
    "                true_out, true_logit = self.disc(true_imgs, true_embed)\n",
    "\n",
    "                activation_fake = torch.mean(fake_logit, 0)\n",
    "                activation_real = torch.mean(true_logit, 0)\n",
    "\n",
    "                gen_loss = self.criterion(fake_out, real_labels)\n",
    "                gen_loss = gen_loss + self.l1_coeff * self.l1loss(fake_imgs, true_imgs) + self.l2loss(activation_fake, activation_real)\n",
    "\n",
    "                gen_loss.backward()\n",
    "                self.gen_optim.step()\n",
    "\n",
    "                # self.cls_gan_optim.step()\n",
    "\n",
    "                # Logging\n",
    "                loss = {}\n",
    "                loss['G_loss'] = gen_loss.item()\n",
    "                loss['D_loss'] = disc_loss.item()\n",
    "\n",
    "                # ---------------------------------------------------------------\n",
    "                # \t\t\t\t\t4. Logging INFO into log_dir\n",
    "                # ---------------------------------------------------------------\n",
    "                if idx % self.log_step == 0:\n",
    "                    end_time = time.time() - start_time\n",
    "                    end_time = datetime.timedelta(seconds=end_time)\n",
    "                    log = \"Elapsed [{}], Epoch [{}/{}], Idx [{}/{}]\".format(end_time, epoch,\n",
    "                                                                         self.num_epochs, idx, len(data_loader))\n",
    "                    for net, loss_value in loss.items():\n",
    "                        log += \", {}: {:.4f}\".format(net, loss_value)\n",
    "                    print (log)\n",
    "                    self.logger.info(log)\n",
    "\n",
    "                \"\"\"\n",
    "                log = \"Epoch [{}/{}], Idx [{}/{}]\".format(epoch, self.num_epochs, idx, len(data_loader))\n",
    "                for net, loss_value in loss.items():\n",
    "                    log += \", {}: {:.4f}\".format(net, loss_value)\n",
    "                \n",
    "                self.logger.info(log)\n",
    "                \"\"\"    \n",
    "\n",
    "                # ---------------------------------------------------------------\n",
    "                # \t\t\t\t\t5. Saving generated images\n",
    "                # ---------------------------------------------------------------\n",
    "                if (idx + 1) % self.sample_step == 0:\n",
    "                    fake_imgs = self.gen(true_embed, fixed_noise)\n",
    "                    concat_imgs = torch.cat((true_imgs, fake_imgs), 2)  # ??????????\n",
    "                    save_path = os.path.join(self.sample_dir, '{}-images.png'.format(idx + 1))\n",
    "                    concat_imgs = (concat_imgs + 1) / 2\n",
    "                    # out.clamp_(0, 1)\n",
    "                    #save_image(concat_imgs.data.cpu(), save_path, nrow=1, padding=0)\n",
    "                    self.save_img_results(true_imgs, fake_imgs, epoch, self.sample_dir)\n",
    "\n",
    "                    print ('Saved real and fake images into {}...'.format(self.sample_dir))\n",
    "\n",
    "                # ---------------------------------------------------------------\n",
    "                # \t\t\t\t6. Saving the checkpoints & final model\n",
    "                # ---------------------------------------------------------------\n",
    "            \n",
    "            end_t = time.time()    \n",
    "            G_path = os.path.join(self.checkpoint_dir, '{}-G.ckpt'.format(epoch))\n",
    "            D_path = os.path.join(self.checkpoint_dir, '{}-D.ckpt'.format(epoch))\n",
    "            torch.save(self.gen.state_dict(), G_path)\n",
    "            torch.save(self.disc.state_dict(), D_path)\n",
    "            print(log)\n",
    "            print('Total Time: {:.2f} sec and Saved model checkpoints into {}...'.format((end_t - start_t), self.checkpoint_dir))        \n",
    "\n",
    "        print ('---------------  Model Training Completed  ---------------')\n",
    "        # Saving final model into final_model directory\n",
    "        G_path = os.path.join(self.final_model, '{}-G.pth'.format('final'))\n",
    "        D_path = os.path.join(self.final_model, '{}-D.pth'.format('final'))\n",
    "        torch.save(self.gen.state_dict(), G_path)\n",
    "        torch.save(self.disc.state_dict(), D_path)\n",
    "        print('Saved final model into {}...'.format(self.final_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d2befbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pickles'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickles\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Variable\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pickles'"
     ]
    }
   ],
   "source": [
    "%tb\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-f')\n",
    "    parser.add_argument('--batch_size', type=int, default=1,\n",
    "                        help='Batch Size')\n",
    "    parser.add_argument('--img_size', type=int, default=64,\n",
    "                        help='Size of the image')\n",
    "    parser.add_argument('--z_dim', type=int, default=100,\n",
    "                        help='Size of the latent variable')\n",
    "    parser.add_argument('--final_model', type=str, default='final_model',\n",
    "                        help='Save INFO into logger after every x iterations')\n",
    "    parser.add_argument('--save_img', type=str, default='test',\n",
    "                        help='Save predicted images')\n",
    "    parser.add_argument('--text_embed_dim', type=int, default=4800,\n",
    "                        help='Size of the embeddding for the captions')\n",
    "    parser.add_argument('--text_reduced_dim', type=int, default=1024,\n",
    "                        help='Reduced dimension of the caption encoding')\n",
    "    parser.add_argument('--text', type=str, help='Input text to be converted into image')\n",
    "    \n",
    "    config = parser.parse_args()\n",
    "#     if not os.path.exists(config.save_img):\n",
    "#         os.makedirs('Data' + config.save_img)\n",
    "\n",
    "    start_time = time.time()\n",
    "    gen = Generator(batch_size=config.batch_size,\n",
    "                    img_size=config.img_size,\n",
    "                    z_dim=config.z_dim,\n",
    "                    text_embed_dim=config.text_embed_dim,\n",
    "                    reduced_text_dim=config.text_reduced_dim)\n",
    "\n",
    "    # Loading the trained model\n",
    "    G_path = os.path.join(config.final_model, '{}-G.pth'.format('final'))\n",
    "    gen.load_state_dict(torch.load(G_path))\n",
    "    # torch.load(gen.state_dict(), G_path)\n",
    "    gen.eval()\n",
    "\n",
    "    z = Variable(torch.randn(config.batch_size, config.z_dim)).cuda()\n",
    "    model = load_model()\n",
    "    text_embed = skipthoughts.encode(model, config.text)\n",
    "    output_img = gen(text_embed, z)\n",
    "    save_image(output_img.cpu(), config.save_img, nrow=1, padding=0)\n",
    "\n",
    "    print ('Generated image save to {}'.format(config.save_img))\n",
    "    print ('Time taken for the task : {}'.format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35872754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visdom import Visdom\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from PIL import ImageDraw, Image, ImageFont\n",
    "import torch\n",
    "import pdb\n",
    "\n",
    "class VisdomPlotter(object):\n",
    "\n",
    "    \"\"\"Plots to Visdom\"\"\"\n",
    "\n",
    "    def __init__(self, env_name='gan'):\n",
    "        self.viz = Visdom()\n",
    "        self.env = env_name\n",
    "        self.plots = {}\n",
    "\n",
    "    def plot(self, var_name, split_name, x, y, xlabel='epoch'):\n",
    "        if var_name not in self.plots:\n",
    "            self.plots[var_name] = self.viz.line(X=np.array([x,x]), Y=np.array([y,y]), env=self.env, opts=dict(\n",
    "                legend=[split_name],\n",
    "                title=var_name,\n",
    "                xlabel=xlabel,\n",
    "                ylabel=var_name\n",
    "            ))\n",
    "        else:\n",
    "            self.viz.updateTrace(X=np.array([x]), Y=np.array([y]), env=self.env, win=self.plots[var_name], name=split_name)\n",
    "\n",
    "    def draw(self, var_name, images):\n",
    "        if var_name not in self.plots:\n",
    "            self.plots[var_name] = self.viz.images(images, env=self.env)\n",
    "        else:\n",
    "            self.viz.images(images, env=self.env, win=self.plots[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11b42c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No traceback available to show.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints created\n",
      "sample created\n",
      "logs created\n",
      "final_model created\n",
      "------------------  Loading images  ------------------\n",
      "Load filenames from: C:\\Base\\file_caption_map.pickle (18818)\n",
      "------------------  Loading captions  ----------------\n",
      "---------------  Loading Skip-thought Model  ---------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py:205\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    204\u001b[0m         warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mWarning\u001b[39;00m)\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m excs_to_catch:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;66;03m# e.g.\u001b[39;00m\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;66;03m#  \"No module named 'pandas.core.sparse.series'\"\u001b[39;00m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;66;03m#  \"Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib\"\u001b[39;00m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xd3 in position 2: ordinal not in range(128)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 125>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    122\u001b[0m     gan\u001b[38;5;241m.\u001b[39mtrain_model()\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 126\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    113\u001b[0m args \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args()\n\u001b[0;32m    115\u001b[0m check_args(args)\n\u001b[1;32m--> 117\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mText2ImageDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    120\u001b[0m gan \u001b[38;5;241m=\u001b[39m GAN_CLS(args, data_loader)\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mText2ImageDataset.__init__\u001b[1;34m(self, data_dir)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir \u001b[38;5;241m=\u001b[39m data_dir\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      6\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mGrayscale(num_output_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m      7\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m      8\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize([\u001b[38;5;241m0.5\u001b[39m], [\u001b[38;5;241m0.5\u001b[39m])])\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_flower_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mText2ImageDataset.load_flower_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     31\u001b[0m embedding_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/file_caption_embedding.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mBase\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m embedding_filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 34\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoded_captions \u001b[38;5;241m=\u001b[39m  embeddings   \n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-------------  Encoding of image captions DONE  -------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py:213\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pc\u001b[38;5;241m.\u001b[39mload(handles\u001b[38;5;241m.\u001b[39mhandle, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;66;03m# e.g. can occur for files written in py27; see GH#28645 and GH#31988\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatin-1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\compat\\pickle_compat.py:272\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fh, encoding, is_verbose)\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;66;03m# \"Unpickler\" has no attribute \"is_verbose\"  [attr-defined]\u001b[39;00m\n\u001b[0;32m    270\u001b[0m     up\u001b[38;5;241m.\u001b[39mis_verbose \u001b[38;5;241m=\u001b[39m is_verbose  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\pickle.py:1208\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1207\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1208\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1209\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m key:\n\u001b[0;32m   1210\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\pickle.py:298\u001b[0m, in \u001b[0;36m_Unframer.read\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%tb\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "def check_dir(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "    print ('{} created'.format(dir_name))\n",
    "\n",
    "\n",
    "def check_args(args):\n",
    "    # Make all directories if they don't exist\n",
    "\n",
    "    # --checkpoint_dir\n",
    "    check_dir(args.checkpoint_dir)\n",
    "\n",
    "    # --sample_dir\n",
    "    check_dir(args.sample_dir)\n",
    "\n",
    "    # --log_dir\n",
    "    check_dir(args.log_dir)\n",
    "\n",
    "    # --final_model dir\n",
    "    check_dir(args.final_model)\n",
    "\n",
    "    # --epoch\n",
    "    assert args.num_epochs > 0, 'Number of epochs must be greater than 0'\n",
    "\n",
    "    # --batch_size\n",
    "    assert args.batch_size > 0, 'Batch size must be greater than zero'\n",
    "\n",
    "    # --z_dim\n",
    "    assert args.z_dim > 0, 'Size of the noise vector must be greater than zero'\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-f')\n",
    "\n",
    "    parser.add_argument_group('Dataset related arguments')\n",
    "    parser.add_argument('--data_dir', type=str, default=\"data\",\n",
    "                        help='Data Directory')\n",
    "\n",
    "    parser.add_argument('--dataset', type=str, default=\"flowers\",\n",
    "                        help='Dataset to train')\n",
    "\n",
    "    parser.add_argument_group('Model saving path and steps related arguments')\n",
    "    parser.add_argument('--log_step', type=int, default=1,\n",
    "                        help='Save INFO into logger after every x iterations')\n",
    "\n",
    "    parser.add_argument('--sample_step', type=int, default=100,\n",
    "                        help='Save generated image after every x iterations')\n",
    "\n",
    "    parser.add_argument('--checkpoint_dir', type=str, default='checkpoints',\n",
    "                        help='Save model checkpoints after every x iterations')\n",
    "\n",
    "    parser.add_argument('--sample_dir', type=str, default='sample',\n",
    "                        help='Save generated image after every x iterations')\n",
    "\n",
    "    parser.add_argument('--log_dir', type=str, default='logs',\n",
    "                        help='Save INFO into logger after every x iterations')\n",
    "\n",
    "    parser.add_argument('--final_model', type=str, default='final_model',\n",
    "                        help='Save INFO into logger after every x iterations')\n",
    "\n",
    "    parser.add_argument_group('Model training related arguments')\n",
    "    parser.add_argument('--num_epochs', type=int, default=200,\n",
    "                        help='Total number of epochs to train')\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int, default=64,\n",
    "                        help='Batch Size')\n",
    "\n",
    "    parser.add_argument('--img_size', type=int, default=64,\n",
    "                        help='Size of the image')\n",
    "\n",
    "    parser.add_argument('--z_dim', type=int, default=100,\n",
    "                        help='Size of the latent variable')\n",
    "\n",
    "    parser.add_argument('--text_embed_dim', type=int, default=4800,\n",
    "                        help='Size of the embeddding for the captions')\n",
    "\n",
    "    parser.add_argument('--text_reduced_dim', type=int, default=1024,\n",
    "                        help='Reduced dimension of the caption encoding')\n",
    "\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.0002,\n",
    "                        help='Learning Rate')\n",
    "\n",
    "    parser.add_argument('--beta1', type=float, default=0.5,\n",
    "                        help='Hyperparameter of the Adam optimizer')\n",
    "\n",
    "    parser.add_argument('--beta2', type=float, default=0.999,\n",
    "                        help='Hyperparameter of the Adam optimizer')\n",
    "\n",
    "    parser.add_argument('--l1_coeff', type=float, default=50,\n",
    "                        help='Coefficient for the L1 Loss')\n",
    "\n",
    "    parser.add_argument('--l2_coeff', type=float, default=100,\n",
    "                        help='Coefficient for the L1 Loss')\n",
    "\n",
    "    parser.add_argument('--resume_epoch', type=int, default=0,\n",
    "                        help='Resume epoch to resume training')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    check_args(args)\n",
    "\n",
    "    dataset = Text2ImageDataset(args.data_dir)\n",
    "    data_loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "    gan = GAN_CLS(args, data_loader)\n",
    "    gan.build_model()\n",
    "    gan.train_model()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be06b4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Base\n",
      "------------------------SKIP THOUGHT LOADING-----------------------\n",
      "Loading model parameters...\n",
      "Compiling encoders...\n",
      "Loading tables...\n",
      "Packing up...\n",
      "uoptions\n",
      "boptions\n",
      "utable\n",
      "btable\n",
      "f_w2v\n",
      "f_w2v2\n",
      "------------------------SKIP THOUGHT LOADING FINISHED-----------------------\n",
      "------------------------GENERATOR LOADING-----------------------\n",
      "------------------------GENERATOR LOADING FINISHED-----------------------\n",
      "['alpha to the right of beta']\n",
      "6\n",
      "[[-0.0121488   0.00848362  0.00015675 ... -0.03058045 -0.01231285\n",
      "  -0.01715059]]\n",
      "torch.Size([1, 4800])\n",
      "['percentage to the right of thunder']\n",
      "6\n",
      "[[-0.00747196  0.00042758 -0.00934522 ... -0.01970895 -0.00348631\n",
      "   0.01339394]]\n",
      "torch.Size([1, 4800])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "\n",
    "\n",
    "import tkinter\n",
    "\n",
    "from tkinter import *\n",
    "from PIL import Image, ImageTk\n",
    "import numpy\n",
    "import random\n",
    "\n",
    "\n",
    "dir_path = (os.path.abspath(os.path.join(os.path.realpath('C:\\Base'))))\n",
    "print(dir_path)\n",
    "sys.path.append(dir_path)\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-f')\n",
    "    parser.add_argument('--batch_size', type=int, default=1,\n",
    "                        help='Batch Size')\n",
    "    parser.add_argument('--img_size', type=int, default=64,\n",
    "                        help='Size of the image')\n",
    "    parser.add_argument('--z_dim', type=int, default=100,\n",
    "                        help='Size of the latent variable')\n",
    "    parser.add_argument('--final_model', type=str, default='final_model',\n",
    "                        help='Save INFO into logger after every x iterations')\n",
    "    parser.add_argument('--save_img', type=str, default='.',\n",
    "                        help='Save predicted images')\n",
    "    parser.add_argument('--text_embed_dim', type=int, default=4800,\n",
    "                        help='Size of the embeddding for the captions')\n",
    "    parser.add_argument('--text_reduced_dim', type=int, default=1024,\n",
    "                        help='Reduced dimension of the caption encoding')\n",
    "    parser.add_argument('--text', type=str, help='Input text to be converted into image')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "config = parse_args()\n",
    "\n",
    "\n",
    "print('------------------------SKIP THOUGHT LOADING-----------------------')\n",
    "model = load_model()\n",
    "encoder = Encoder(model)\n",
    "print('------------------------SKIP THOUGHT LOADING FINISHED-----------------------')\n",
    "\n",
    "print('------------------------GENERATOR LOADING-----------------------')\n",
    "\n",
    "gen = Generator(batch_size=config.batch_size,\n",
    "                    img_size=config.img_size,\n",
    "                    z_dim=config.z_dim,\n",
    "                    text_embed_dim=config.text_embed_dim,\n",
    "                    reduced_text_dim=config.text_reduced_dim)\n",
    "\n",
    "gen.cuda()\n",
    "# Loading the trained model\n",
    "G_path = os.path.join('C:/Base/final/', 'final_model-G.ckpt')\n",
    "\n",
    "\n",
    "gen.load_state_dict(torch.load(G_path))\n",
    "gen.eval()\n",
    "print('------------------------GENERATOR LOADING FINISHED-----------------------')\n",
    "\n",
    "output_dir = './'    \n",
    "\n",
    "\n",
    "top = Tk()\n",
    "top.title('Reverse Image Captioning')\n",
    "top.geometry('500x500')\n",
    "\n",
    "top_row = Frame(top).grid(row=0)\n",
    "\n",
    "left = Frame(top_row).grid(row=0, column=0)\n",
    "L1 = Label(left, text=\"Enter the image description:\").grid(row=0)\n",
    "E1 = Entry(left)\n",
    "E1.grid(row=1)\n",
    "\n",
    "right = Frame(top_row).grid(row=0, column=1)\n",
    "\n",
    "canvas = Canvas(right, width=300,height=300, bd=0,bg='white')\n",
    "canvas.grid(row=0, column=1)\n",
    "\n",
    "def GenerateImage():\n",
    "    z = torch.randn(config.batch_size, config.z_dim)\n",
    "    z = z.cuda()\n",
    "    text_input = E1.get()\n",
    "    if (len(text_input) >= 10):\n",
    "        text_input = text_input.lower()\n",
    "        text_input = [text_input]\n",
    "        print(text_input)\n",
    "        text_embedding = encoder.encode(text_input)\n",
    "        print(text_embedding)\n",
    "        text_embedding = torch.from_numpy(text_embedding)\n",
    "        text_embedding = text_embedding.cuda()\n",
    "        print(text_embedding.shape)\n",
    "        output_img = gen(text_embedding, z)\n",
    "        save_name = 'output.png'\n",
    "        \n",
    "        # fake.data is still [-1, 1]\n",
    "        vutils.save_image(output_img.data, save_name, normalize=True)\n",
    "            \n",
    "        load = Image.open(save_name)\n",
    "        w, h = load.size\n",
    "        load = load.resize((256, 256))\n",
    "        imgfile = ImageTk.PhotoImage(load)\n",
    "        \n",
    "        canvas.image = imgfile  # <--- keep refe~rence of your image\n",
    "        canvas.create_image(2,2,anchor='nw',image=imgfile)\n",
    "    \n",
    "    E1.delete(0, END)\n",
    "\n",
    "\n",
    "submit_button = Button(top, text ='Generate Image', command = GenerateImage)\n",
    "submit_button.grid(row=2, column=0)\n",
    "\n",
    "submit_button = Button(top, text ='Exit', command = top.quit)\n",
    "submit_button.grid(row=2, column=1)\n",
    "\n",
    "top.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655b8a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ade75fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8110ceff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
